This is the report of N-gram model by Zhi Qu.
Defect of this work:
    I didn't have enough time to beautify my codes, so it looks like very disorder.
    But I will explain the logic as follows:

1. This n-gram model codes considered any N but excluding N = 1, since the logic of unigram is different from others.

2. Reuslts:
    Evaluation of loss = 9.492
    Evaluated by loss = loss_total / number of words
    incorrect sentence whose loss > 600 in test file:
    scores_idx_3 = 619.633
    scores_idx_139 = 727.861
    scores_idx_145 = 674.544
    scores_idx_169 = 640.122

3. Logic of implementation and codes explanation
    3.1 function: def _total_num_words
         it's for counting total number of words in file.
    3.2 function: def _read_txt_
         ir's for reading data from file
    3.3 function: _count_N_
         it's for counting the frequency of N-gram (windows with N size)
         For convenience, I count the frequency from 1 to N (3 I defined).
    3.4 Block : Witten-Bell Smoothing
         I have implemented this one, but did not use it.
    3.5 Block: count_continuation 
         It's for for computing P_continuation.
         To search the combination of N-gram whose N-1_gram is as same as this window.
    3.6 function: _compute_p_kn_
         P_kn = p_kn + P_continuation
         p_kn = max(count(N_gram) - d, 0)/count(N-1_gram)
         P_continuation = lambda * p_continuation
         lambda: d/ count(N-1_gram) * count(N_gram)
     3.7 function: _p_
         It's for computing P by using function 3.6
         for unkown word, p = lambda_unk * (1 / number of words)
